{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4372b9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b837a7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "FileData = pd.read_csv(\"data2.txt\",header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95229f5",
   "metadata": {},
   "source": [
    "# Plotting the data (i.e., x-axis for the 1st column, y-axis for the 2nd column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d8607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "FileData.columns=[\"x\",\"y\"]\n",
    "FileData.plot(kind=\"scatter\",x='x',y='y',color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c942b633",
   "metadata": {},
   "outputs": [],
   "source": [
    "FileData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689d1410",
   "metadata": {},
   "source": [
    "# Normal Equation to find coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d983f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = FileData.iloc[:,0].values\n",
    "Y = FileData.iloc[:,1].values\n",
    "Xbias = np.c_[np.ones((97,1)),X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dea4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "thetaCoefficients = np.linalg.inv(Xbias.T @ Xbias) @ Xbias.T @ Y\n",
    "print(thetaCoefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49b0408",
   "metadata": {},
   "outputs": [],
   "source": [
    "XTest = np.array([[2],[30]])\n",
    "XTestBias = np.c_[np.ones((2,1)), XTest]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b4f991",
   "metadata": {},
   "source": [
    "Plotting the normal equation line, on the dataset, using the calculated parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cdaec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "YPredict = XTestBias @ thetaCoefficients\n",
    "plt.plot(XTest,YPredict,'b')\n",
    "plt.scatter(X,Y, color = '#88c999')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4548189e",
   "metadata": {},
   "source": [
    "# Spliting dataset into 80% training data and 20% testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552f6984",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData = FileData.sample(frac=0.8, random_state = 30)\n",
    "testingData = FileData.drop(trainingData.index)\n",
    "print(len(trainingData),len(testingData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81fb07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrain = trainingData[\"x\"]\n",
    "XTest = testingData[\"x\"]\n",
    "YTraining = trainingData[\"y\"]\n",
    "YTesting = testingData[\"y\"]\n",
    "XTrainBias = np.c_[np.ones((len(XTrain), 1)), XTrain]\n",
    "XTestBias = np.c_[np.ones((len(XTest), 1)), XTest]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d32f984",
   "metadata": {},
   "source": [
    "Below function performs batch gradient descent which uses the training data. The gradient for updating theta is calculated using the training data. The mse for both the training data and the test data is then determined using the new theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b972b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BatchGradientDescent(LearningRate, theta, iterations, XTrain, XTest, YTraining, YTesting):\n",
    "    theta = random_theta\n",
    "    for i in range(iterations):\n",
    "        j = (2 / numberOfTrainSamples)\n",
    "        gradTrain = j * XTrain.T @ (XTrain @ theta - YTraining)\n",
    "        theta = theta - LearningRate * gradTrain\n",
    "        MSETraining = (1 / numberOfTrainSamples) * ((YTraining - XTrain @ theta).T @ (YTraining - XTrain @ theta))\n",
    "        MSEListTraining.append(float(MSETraining))\n",
    "        MSETesting = (1 / numberOfTestSamples) * ((YTesting - XTest @ theta).T @ (YTesting - XTest @ theta))\n",
    "        MSEListTesting.append(float(MSETesting))\n",
    "    return theta, MSEListTraining, MSEListTesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeabdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "YTraining = YTraining.values.tolist()\n",
    "YTesting = YTesting.values.tolist()\n",
    "YTraining = np.array(YTraining)\n",
    "YTesting = np.array(YTesting)\n",
    "YTraining = YTraining.reshape(-1, 1)  \n",
    "YTesting = YTesting.reshape(-1, 1)    \n",
    "LearningRate = 0.01\n",
    "iterations = 500\n",
    "numberOfTrainSamples = len(XTrain)\n",
    "numberOfTestSamples = len(XTest)\n",
    "MSEListTraining = []\n",
    "MSEListTesting = []\n",
    "random_theta = np.random.randn(2, 1)\n",
    "\n",
    "\n",
    "theta, MSEListTraining, MSEListTesting = BatchGradientDescent(LearningRate, random_theta, iterations, XTrainBias, XTestBias, YTraining, YTesting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9f902d",
   "metadata": {},
   "source": [
    "MSE vs Iterations in Batch training descent for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62c547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(MSEListTraining)\n",
    "plt.title(\"MSE vs Iterations for training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64732b37",
   "metadata": {},
   "source": [
    "MSE vs Iterations in Batch training descent for testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c731bca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(MSEListTesting)\n",
    "plt.title(\"MSE vs Iterations for testing data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d142da",
   "metadata": {},
   "source": [
    "Below is the function that performs Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e606576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def StochasticGradientDescent(LearningRate, theta, epochs, XTrain, XTest, YTraining, YTesting):\n",
    "    theta = random_theta\n",
    "    for epoch in range(epochs):\n",
    "        MSETrainSum = 0\n",
    "        for i in range(num_samples_train):\n",
    "            random_index = np.random.randint(num_samples_train)\n",
    "            trainXi = XTrain[random_index : random_index + 1]\n",
    "            trainYi = YTraining[random_index : random_index + 1]\n",
    "            grad = (trainXi @ theta - trainYi)\n",
    "            gradients = 2 * (trainXi.T @ grad)\n",
    "            theta = theta - LearningRate * gradients\n",
    "            MSETrainSum += ((trainYi - (trainXi @ theta)) ** 2)\n",
    "            \n",
    "        MSETraining = MSETrainSum / num_samples_train\n",
    "        MSEListTraining.append(float(MSETraining))\n",
    "        \n",
    "        MSETesting = (1 / numberOfTestSamples) * ((YTesting - XTest @ theta).T @ (YTesting - XTest @ theta))\n",
    "        MSEListTesting.append(float(MSETesting))\n",
    "        \n",
    "    return theta, MSEListTraining, MSEListTesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab687cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "LearningRate = 0.01\n",
    "num_samples_train = len(XTrain)\n",
    "num_samples_test = len(XTest)\n",
    "MSEListTraining = []\n",
    "MSEListTesting = []\n",
    "\n",
    "theta, MSEListTraining, MSEListTesting = StochasticGradientDescent(LearningRate, random_theta, epochs, XTrainBias, XTestBias, YTraining, YTesting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1d8c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(MSEListTraining)\n",
    "plt.title(\"MSE vs Iterations for training data in Stochastic\", color =\"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1117fd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(MSEListTesting)\n",
    "plt.title(\"MSE vs Iterations for testing data in Stochastic\", color =\"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04866b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LearningRate_list = [0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009, 0.01]\n",
    "MSEListBatch = []\n",
    "MSEListStochastic = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29b1461",
   "metadata": {},
   "outputs": [],
   "source": [
    "for LearningRate in LearningRate_list:\n",
    "    MSEListTraining_batch = []\n",
    "    \n",
    "    _, MSEListTraining_batch, _ = BatchGradientDescent(LearningRate, random_theta, iterations, XTrainBias, XTestBias, YTraining, YTesting)\n",
    "    \n",
    "    MSEListBatch.append(MSEListTraining_batch[-1])\n",
    "    \n",
    "\n",
    "    \n",
    "for LearningRate in LearningRate_list:\n",
    "    MSEListTraining_stochastic = []\n",
    "    \n",
    "    _, MSEListTraining_stochastic, _ = StochasticGradientDescent(LearningRate, random_theta, epochs, XTrainBias, XTestBias, YTraining, YTesting)\n",
    "    \n",
    "    MSEListStochastic.append(MSEListTraining_stochastic[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895d2d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(LearningRate_list, MSEListBatch)\n",
    "plt.title(\"Learning rate vs MSE (Batch)\", color =\"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0fb50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(LearningRate_list, MSEListStochastic)\n",
    "plt.title(\"Learning rate vs MSE (Stochastic)\", color =\"green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfc7e82",
   "metadata": {},
   "source": [
    "### 3B) We can observe that the batch gradient descent has more accuracy as the error rate decreases smoothly while in case of stochastic,it is less reliable but it converges faster as compared to batch gradient descent. In case of batch gradient regarding termination condition, it smooths out but after a certain point it becomes flat so we can terminate it at that point. While in case of Stochastic, at start, it has a sharp drop. After that point we can terminate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
